{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jakob/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/Jakob/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/Jakob/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/Jakob/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#### Libraries ####\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import ipywidgets as widgets\n",
    "import qgrid\n",
    "import pandas_profiling\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Settings ####\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 400) \n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function Library ####\n",
    "\n",
    "def print_bold(string):\n",
    "    print('\\033[1m' + string )\n",
    "    print('\\033[0m')\n",
    "\n",
    "### Inital steps ###\n",
    "def examine_df(df):\n",
    "    print(df.info())\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    print(df.describe())\n",
    "    \n",
    "    return pandas_profiling.ProfileReport(df)\n",
    "\n",
    "def write_result_to_csv(df):\n",
    "    df.to_csv('result.csv', sep=',')\n",
    "    \n",
    "def distribution_plot(df, column_name):\n",
    "    sns.distplot(df[column_name])\n",
    "    return plt.figure()\n",
    "    \n",
    "def joint_plot(df,x,y):\n",
    "    return sns.jointplot(x = x, y=y, data = df, kind = 'reg')\n",
    "\n",
    "def pair_plot(df):\n",
    "    return sns.pairplot(df, kind = 'reg')\n",
    "\n",
    "### Pre-Processing ###\n",
    "\n",
    "def seperate_components(df, column_of_y):\n",
    "    print('Note: let x,y = function to define globally')\n",
    "    time.sleep(2)\n",
    "    array = df.values\n",
    "    if ((int(len(array[0]))) - 1) != column_of_y:\n",
    "        print('Warning: adapt function if y not last column!')\n",
    "    X = array[:,0:column_of_y]\n",
    "    Y = array[:,column_of_y]\n",
    "    return X,Y\n",
    "\n",
    "def rescale(x):\n",
    "    print('Note: let rescaledX = rescale(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    rescaledX=scaler.fit_transform(x)\n",
    "    return rescaledX\n",
    "\n",
    "def standardize(x):\n",
    "    print('Note: let standardizedX = standardize(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    scaler= StandardScaler().fit(x)\n",
    "    rescaledX = scaler.transform(x)\n",
    "    return standardizedX\n",
    "\n",
    "def normalize(x):\n",
    "    print('Note: let normalizedX = normalize(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    normalizedX = Normalizer().fit_transform(x)\n",
    "    return normalizedX\n",
    "\n",
    "def binarize(x,threshold):\n",
    "    print('Note: let binarizedX = binarize(X) to define globally and set threshold to value required')\n",
    "    time.sleep(2)\n",
    "    binaryX = Binarizer(threshold=threshold).fit_transform(x)\n",
    "    return binaryX\n",
    "\n",
    "def encode(df,name_of_column,new_name):\n",
    "    print('Note: let df = encode(X,name_of_column,new_name) to define globally')\n",
    "    time.sleep(2)\n",
    "    df[name_of_column]=LabelEncoder().fit_transform(df[new_name])\n",
    "    return df\n",
    "    \n",
    "def get_dummies(df, column_name):\n",
    "    print('Note: let df = get_dummies(df,name_of_column) to define globally')\n",
    "    time.sleep(2)\n",
    "    print(df[column_name].unique())\n",
    "    gen_features = pd.get_dummies(df[column_name],prefix = column_name, prefix_sep= '_',drop_first = True)\n",
    "    df = pd.concat([df,gen_features], axis=1)\n",
    "    df = df.drop([column_name], axis=1)\n",
    "    return df\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "def univariate_chi(x,y,df,target_var, k=4):\n",
    "    test = SelectKBest(score_func=chi2,k=k)\n",
    "    fit = test.fit(x,y)\n",
    "    print_bold('Univariate Scores')\n",
    "    score = list(fit.scores_)\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    results.loc[''] = score\n",
    "    print(f'The {k} attributes with highest scores are: ')\n",
    "    count = 1\n",
    "    while count <= k:\n",
    "        max_value = results.idxmax(axis=1)\n",
    "        print(f'{count}: ' + max_value.values)\n",
    "        results = results.drop(columns = max_value.values)\n",
    "        count += 1\n",
    "    print('------------')\n",
    "\n",
    "def recursive_elimination(x,y,df,target_var, k=3):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    \n",
    "    rfe = RFE(model,k)\n",
    "    fit = rfe.fit(x,y)\n",
    "    print_bold(f'Recursive Scores')\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    score = list(fit.ranking_)\n",
    "    score = list(map(int, score))\n",
    "    results = pd.DataFrame(columns = columns)\n",
    "    results.loc[''] = score\n",
    "    print(f'The {k} attributes with highest scores are: ')\n",
    "    count = 1\n",
    "    while count <= k:\n",
    "        min_value = results.astype('float64').idxmin(axis=1)\n",
    "        print(f'{count}: ' + min_value.values)\n",
    "        results = results.drop(columns = min_value.values)\n",
    "        count += 1\n",
    "    print('------------')\n",
    "    \n",
    "def pca(x,k=3):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca_fit = pca.fit(x)\n",
    "    print(f\"Explained variance: {pca_fit.explained_variance_ratio_}\")\n",
    "    print()\n",
    "    print(\"Principal Components have little resemblance to the source data attributes\")\n",
    "    print()\n",
    "    print(pca_fit.components_)\n",
    "\n",
    "def extra_trees(x,y,df,target_var,estimators=100):\n",
    "    model = ExtraTreesClassifier(n_estimators=estimators)\n",
    "    model.fit(x,y)\n",
    "    print_bold('Feature Importance Scores')\n",
    "    score = list(model.feature_importances_)\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    results.loc[''] = score\n",
    "    np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "    print(f'The importance of attributes in descending order: ')\n",
    "    print()\n",
    "    print(round((results.max().sort_values(ascending=False)),3))\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Import ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculations ### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
