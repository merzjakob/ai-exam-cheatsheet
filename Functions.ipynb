{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries ####\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import ipywidgets as widgets\n",
    "import qgrid\n",
    "import pandas_profiling\n",
    "import time\n",
    "import sklearn\n",
    "import math\n",
    "import chart_studio.plotly as py\n",
    "from IPython.display import HTML\n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG, display\n",
    "from ipywidgets import interactive\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#Model Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "#Performance Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#clustering\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn import metrics\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#ensembles\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Settings ####\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 400) \n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function Library ####\n",
    "\n",
    "def print_bold(string):\n",
    "    print()\n",
    "    print('\\033[1m' + string )\n",
    "    print('\\033[0m')\n",
    "    \n",
    "def visualizer_res(result_df):\n",
    "    print('Put results in first column and description in second')\n",
    "    sleep(2)\n",
    "    columns = (list(result_df.columns.values))\n",
    "    plt.figure(figsize=(15,9))\n",
    "    sns.boxplot(data=resall, x=columns[1], y=columns[0])\n",
    "    sns.swarmplot(data=resall, x=columns[1], y=columns[0], color=\"royalblue\")\n",
    "\n",
    "### Inital steps ###\n",
    "def nan_examination(df):\n",
    "    # get size of dataframe\n",
    "    df.shape\n",
    "    # absolute missing values per column\n",
    "    df.isnull().sum()\n",
    "    # determine % of nan in each column\n",
    "    df.isnull().sum() / df.shape[0] * 100.00\n",
    "    # visualize percentages\n",
    "    (df.isnull().sum() / df.shape[0] * 100.00).plot(kind='barh')\n",
    "    \n",
    "def categoricals(df):\n",
    "    for column in df.select_dtypes('object'):\n",
    "        print(f'{column:17s}: {df[column].unique()}')\n",
    "\n",
    "def check_na(df):\n",
    "    for column in df:\n",
    "        if df[column].isnull().any():\n",
    "            print('{0} has {1} na out of {2} equal to {3}%'.format(column, df[column].isnull().sum(),df.shape[0],df[column].isnull().sum()/df.shape[0]*100))\n",
    "    \n",
    "def examine_df(df):\n",
    "    print(df.info())\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    print(df.describe())\n",
    "    \n",
    "    return pandas_profiling.ProfileReport(df)\n",
    "\n",
    "def write_result_to_csv(df):\n",
    "    df.to_csv('result.csv', sep=',')\n",
    "    \n",
    "def distribution_plot(df, column_name):\n",
    "    sns.distplot(df[column_name].notnull())\n",
    "    return plt.figure()\n",
    "    \n",
    "def joint_plot(df,x,y):\n",
    "    return sns.jointplot(x = x, y=y, data = df, kind = 'reg')\n",
    "\n",
    "def pair_plot(df):\n",
    "    return sns.pairplot(df, kind = 'reg')\n",
    "\n",
    "### Pre-Processing ###\n",
    "\n",
    "def seperate_components(df, column_of_y):\n",
    "    X = df.drop(column_of_y, axis=1).values\n",
    "    y = df[column_of_y].values\n",
    "    return X,y\n",
    "\n",
    "def cut_category(column,arr_of_cutoff,labels,new_col_name, position):\n",
    "    category = pd.cut(df[column],bins=arr_of_cutoff,labels=labels)\n",
    "    df.insert(position,new_col_name,category)\n",
    "    return df\n",
    "\n",
    "def rescale(x):\n",
    "    print('Note: let rescaledX,x_scaled_fit = rescale(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    x_scaled_fit = scaler.fit(x)\n",
    "    rescaledX=scaler.fit_transform(x)\n",
    "    return rescaledX, x_scaled_fit\n",
    "\n",
    "def standardize(x):\n",
    "    print('Note: let standardizedX = standardize(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    scaler= StandardScaler().fit(x)\n",
    "    rescaledX = scaler.transform(x)\n",
    "    return standardizedX\n",
    "\n",
    "def normalize(x):\n",
    "    print('Note: let normalizedX = normalize(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    normalizedX = Normalizer().fit_transform(x)\n",
    "    return normalizedX\n",
    "\n",
    "def binarize(x,threshold):\n",
    "    print('Note: let binarizedX = binarize(X) to define globally and set threshold to value required')\n",
    "    time.sleep(2)\n",
    "    binaryX = Binarizer(threshold=threshold).fit_transform(x)\n",
    "    return binaryX\n",
    "\n",
    "def encode(df,name_of_column,new_name):\n",
    "    print('Note: let df = encode(X,name_of_column,new_name) to define globally')\n",
    "    time.sleep(2)\n",
    "    df[name_of_column]=LabelEncoder().fit_transform(df[new_name])\n",
    "    return df\n",
    "    \n",
    "def get_dummies(df, column_name):\n",
    "    print('Note: let df = get_dummies(df,name_of_column) to define globally')\n",
    "    time.sleep(2)\n",
    "    print(df[column_name].unique())\n",
    "    gen_features = pd.get_dummies(df[column_name],prefix = column_name, prefix_sep= '_',drop_first = True)\n",
    "    df = pd.concat([df,gen_features], axis=1)\n",
    "    df = df.drop([column_name], axis=1)\n",
    "    return df\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "def univariate_chi(x,y,df,target_var, k=4):\n",
    "    test = SelectKBest(score_func=chi2,k=k)\n",
    "    fit = test.fit(x,y)\n",
    "    print_bold('Univariate Scores')\n",
    "    score = list(fit.scores_)\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    results.loc[''] = score\n",
    "    print(f'The {k} attributes with highest scores are: ')\n",
    "    count = 1\n",
    "    while count <= k:\n",
    "        max_value = results.idxmax(axis=1)\n",
    "        print(f'{count}: ' + max_value.values)\n",
    "        results = results.drop(columns = max_value.values)\n",
    "        count += 1\n",
    "    print('------------')\n",
    "\n",
    "def recursive_elimination(x,y,df,target_var, k=3):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    \n",
    "    rfe = RFE(model,k)\n",
    "    fit = rfe.fit(x,y)\n",
    "    print_bold(f'Recursive Scores')\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    score = list(fit.ranking_)\n",
    "    score = list(map(int, score))\n",
    "    results = pd.DataFrame(columns = columns)\n",
    "    results.loc[''] = score\n",
    "    print(f'The {k} attributes with highest scores are: ')\n",
    "    count = 1\n",
    "    while count <= k:\n",
    "        min_value = results.astype('float64').idxmin(axis=1)\n",
    "        print(f'{count}: ' + min_value.values)\n",
    "        results = results.drop(columns = min_value.values)\n",
    "        count += 1\n",
    "    print('------------')\n",
    "    \n",
    "def pca(x,k=3):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca_fit = pca.fit(x)\n",
    "    print(f\"Explained variance: {pca_fit.explained_variance_ratio_}\")\n",
    "    print()\n",
    "    print(\"Principal Components have little resemblance to the source data attributes\")\n",
    "    print()\n",
    "    print(pca_fit.components_)\n",
    "\n",
    "def extra_trees(x,y,df,target_var,estimators=100):\n",
    "    model = ExtraTreesClassifier(n_estimators=estimators)\n",
    "    model.fit(x,y)\n",
    "    print_bold('Feature Importance Scores')\n",
    "    score = list(model.feature_importances_)\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    results.loc[''] = score\n",
    "    np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "    print(f'The importance of attributes in descending order: ')\n",
    "    print()\n",
    "    print(round((results.max().sort_values(ascending=False)),3))\n",
    "    print('------------')\n",
    "    \n",
    "### Model Evaluation ###\n",
    "\n",
    "def similarity_of_split(train,test,target_var):\n",
    "    print('TBD')\n",
    "    \n",
    "def test_split(x,y,test_size, seed=7):\n",
    "    #print('Note: let X_train, X_test, Y_train, Y_test = test_split(X,Y, 0.3) to define globally')\n",
    "    time.sleep(2)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size = test_size,random_state = 7)\n",
    "    \n",
    "    # Let's do the log regresssion\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_train,Y_train)\n",
    "\n",
    "    # Now let's find the accurary with the test split\n",
    "    result = model.score(X_test, Y_test)\n",
    "    print(f'Test: {test_size} Train: {1-test_size}')\n",
    "    print(f'Accuracy {result*100:5.3f}')\n",
    "    print('-------------')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def k_fold_cross_val(x,y,splits=10,scoring = 'accuracy',add_info= False):\n",
    "    kfold= StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model= LogisticRegression(solver=\"liblinear\")\n",
    "    if add_info:\n",
    "        scoring = {'accuracy': 'accuracy',\n",
    "           'recall': 'recall',\n",
    "           'precision': 'precision',\n",
    "           'f1': 'f1'}\n",
    "        results = cross_validate(model, x, y, scoring=scoring, cv=kfold)\n",
    "        print(f'Logistic regression, k-fold {splits:d}')\n",
    "        print(f'Accuracy {results[\"test_accuracy\"].mean()*100:.3f}%')\n",
    "        print(f'Precision {results[\"test_precision\"].mean()*100:.3f}%')\n",
    "        print(f'Recall {results[\"test_recall\"].mean()*100:.3f}%')\n",
    "        print(f'F1 {results[\"test_f1\"].mean()*100:.3f}%')\n",
    "    else:\n",
    "        scoring=scoring\n",
    "        results = cross_val_score(model, x,y,cv=kfold,scoring=scoring)\n",
    "        print(f'Logistic regression, k-fold {splits:d} - {scoring}')\n",
    "        if scoring == 'accuracy':\n",
    "            print(f'{results.mean()*100:5.3f}% ({results.std()*100:5.3f}%)')\n",
    "        else:\n",
    "            print(f'{results.mean():5.3f}')\n",
    "    return model\n",
    "\n",
    "              \n",
    "def leave_one_out(x,y):\n",
    "    loo= LeaveOneOut()\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    \n",
    "    results = cross_val_score(model, x,y,cv=loo)\n",
    "    print(f'Logistic regression, Leave one out - Accuracy {results.mean()*100:5.3f}% ({results.std()*100:5.3f}%)')\n",
    "\n",
    "def repeated_test_train(x,y, test_size,repetitions=10):\n",
    "    shuffle=ShuffleSplit(n_splits=repetitions,test_size=test_size, random_state=7)\n",
    "    model= LogisticRegression(solver=\"liblinear\")\n",
    "    res = cross_val_score(model,x,y,cv=shuffle)\n",
    "    \n",
    "    print(f'Log Regression - Repeated Test-Train {nrepeat:d} - Accuracy {res.mean()*100:5.3f}% {res.std()*100:5.3f}%')\n",
    "    \n",
    "\n",
    "### Performance Metrics ###\n",
    "\n",
    "def precision_recall_curve(x,y,test_size):\n",
    "    model = k_fold_cross_val(x,y,add_info=True)\n",
    "              \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "    \n",
    "    viz=PrecisionRecallCurve(model)\n",
    "    viz.fit(X_train, Y_train)\n",
    "    viz.score(X_test,Y_test)\n",
    "    viz.show()\n",
    "\n",
    "def area_under_roc(x,y,test_size):\n",
    "    model = k_fold_cross_val(x,y, scoring='roc_auc')\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "\n",
    "    viz=ROCAUC(model, classes=[0,1])\n",
    "    viz.fit(X_train, Y_train)\n",
    "    viz.score(X_test,Y_test)\n",
    "    viz.show()\n",
    "\n",
    "def con_matrix(x,y,test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    c_matrix=confusion_matrix(y_test, y_predicted)\n",
    "    print_bold(\"Confusion Matrix\")\n",
    "\n",
    "    print()\n",
    "    print(f'Accuracy {model.score(X_test, y_test)*100:.5f}')\n",
    "    print(f'Accuracy check with conf. matrix {(c_matrix[0,0]+c_matrix[1,1])/c_matrix.sum()*100:.5f}')\n",
    "\n",
    "    cm = ConfusionMatrix(model, classes=[\"Not present\",\"Present\"])\n",
    "    # cm.fit(X_train, y_train)  #only if the model is not fitted\n",
    "\n",
    "    cm.score(X_test, y_test)\n",
    "    cm.show()\n",
    "\n",
    "def class_report(x,y, test_size=0.3):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "    \n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_predicted = model.predict(X_test)\n",
    "\n",
    "    report = classification_report(Y_test, Y_predicted, digits=5)\n",
    "\n",
    "    print(f'Accuracy {model.score(X_test, Y_test)*100:.5f}')\n",
    "    print()\n",
    "    print(report)\n",
    "              \n",
    "def mean_abs_error(X,Y):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=7, shuffle= True)\n",
    "    model = LinearRegression()\n",
    "    scoring = \"neg_mean_absolute_error\"\n",
    "    res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    print(f'Linear Regression, MAE: {res.mean():.3f} ({res.std():.3f})')\n",
    "\n",
    "def mean_squared_error(X,Y):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=7, shuffle= True)\n",
    "    model = LinearRegression()\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    print(f'Linear Regression, MSE: {res.mean():.3f} ({res.std():.3f})')\n",
    "    print(f'Linear Regression, MSE: {math.sqrt(abs(res.mean())):.3f} ({math.sqrt(res.std()):.3f})')\n",
    "\n",
    "def r_2(X,Y):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=7, shuffle= True)\n",
    "    model = LinearRegression()\n",
    "              \n",
    "    scoring = \"r2\"\n",
    "    res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    print(f'Linear Regression, R2: {res.mean():.3f} ({res.std():.3f})')\n",
    "\n",
    "### Clustering ###\n",
    "\n",
    "def scale_cluster_df(x):\n",
    "    x_scaled,x_scaled_fit = rescale(x)\n",
    "    X_scaled = pd.DataFrame(x_scaled,columns=x.columns)\n",
    "    return X_scaled,x_scaled_fit        \n",
    "              \n",
    "def elbow(x_scaled):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    model= KMeans()\n",
    "    \n",
    "    visualizer = KElbowVisualizer(model, k=(1,8))\n",
    "    visualizer.fit(x_scaled)       \n",
    "    visualizer.show() \n",
    "     \n",
    "def cluster_model(x_scaled, clusters):\n",
    "    model=MiniBatchKMeans(n_clusters=clusters).fit(x_scaled)\n",
    "    return model\n",
    "              \n",
    "def silhoutte_visual(x_scaled, model):\n",
    "    plt.figure(figsize=(12,9))\n",
    "\n",
    "    visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n",
    "    visualizer.fit(x_scaled)      \n",
    "    visualizer.show()\n",
    "\n",
    "def inter_cluster_dist(x_scaled, model):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    visualizer = InterclusterDistance(model, min_size=10000)\n",
    "    visualizer.fit(x_scaled)\n",
    "    visualizer.show()     \n",
    "\n",
    "def accuracy_clustering(x_scaled, y, model):\n",
    "    model=MiniBatchKMeans(n_clusters=3)\n",
    "\n",
    "    model.fit(X_scaled)\n",
    "    \n",
    "    print(\" ---- Accuracy Scores ----\")\n",
    "\n",
    "    acc_score=accuracy_score(y.values,model.predict(x_scaled))\n",
    "    print(f'Accuracy {acc_score*100:.3f}')\n",
    "\n",
    "def silh_sco(x_scaled, model):\n",
    "    she=silhouette_score(x_scaled, model.labels_, metric='euclidean')\n",
    "    print(f'Silhouette score {she:5f}')\n",
    "\n",
    "def centroid_meaning(model, x, x_scaled):\n",
    "    model.labels_\n",
    "    model.cluster_centers_\n",
    "\n",
    "    centroids_rescaled = pd.DataFrame(model.cluster_centers_, columns=x.columns)\n",
    "    # rescale to original\n",
    "    centroids_original = pd.DataFrame(x_scaled_fit.inverse_transform(model.cluster_centers_),columns=x.columns)\n",
    "    print_bold('Originally scaled centroids')\n",
    "    return centroids_original\n",
    "              \n",
    "def aggl_cluster(x_scaled,clusters,y):\n",
    "    plt.figure(figsize=(17,9))\n",
    "\n",
    "    # create dendrogram\n",
    "    dn = sch.dendrogram(sch.linkage(x_scaled, method='ward'), no_labels=True)\n",
    "    plt.show()\n",
    "    # create clusters\n",
    "    hc = AgglomerativeClustering(n_clusters=clusters, affinity = 'euclidean', linkage = 'ward')\n",
    "    y_hc = hc.fit_predict(X_scaled)\n",
    "    hc.labels_\n",
    "    y.values\n",
    "    dk={0:2,1:0,2:1}\n",
    "    acc_score=accuracy_score(list(map(lambda x:dk[x],y.values)),hc.labels_)\n",
    "    print(f'Accuracy {acc_score*100:.3f}')\n",
    "\n",
    "              \n",
    "### Classification ###\n",
    "              \n",
    "def log_regression(x,y,splits):\n",
    "    #returns model and results\n",
    "    kfold= StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    results=cross_val_score(model, x, y, cv=kfold)\n",
    "    print(f'Logistic Regression - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "    return model, results\n",
    "              \n",
    "def lda(x,y,splits):\n",
    "    #returns model and results\n",
    "    kfold= StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model= LinearDiscriminantAnalysis()\n",
    "    results=cross_val_score(model, x, y, cv=kfold)\n",
    "    print(f'LDA - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "    return model,results\n",
    "\n",
    "def k_nn_k_nearest(x,y,splits):\n",
    "    #returns model and results\n",
    "    kfold= StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model=KNeighborsClassifier()\n",
    "    results=cross_val_score(model, x, y, cv=kfold)\n",
    "    print(f'KNN - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "    return model, results\n",
    "              \n",
    "def naive_bayes(x,y,splits):\n",
    "    #returns model and results\n",
    "    kfold=StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model=GaussianNB()\n",
    "    results=cross_val_score(model, x, y, cv=kfold)\n",
    "    print(f'Naive Bayes - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "    return model, results\n",
    "\n",
    "def decision_tree(x,y,splits):\n",
    "    kfold=StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model=DecisionTreeClassifier(class_weight=\"balanced\", random_state=seed)\n",
    "    results=cross_val_score(model, x, y, cv=kfold)\n",
    "    print(f'Decision Tree - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "    return model, results\n",
    "    \n",
    "def decision_tree_vis(df,model,x,y,zero_class, one_class):\n",
    "    model.fit(x,y)\n",
    "    graph=Source(tree.export_graphviz(model,\n",
    "        out_file=None,      \n",
    "        feature_names=df.columns[0:-1],\n",
    "        class_names=[zero_class,one_class],\n",
    "        filled=True,\n",
    "        rounded=True))\n",
    "\n",
    "    display(SVG(graph.pipe(format=\"svg\")))\n",
    "              \n",
    "def support_vector_machine(x,y,splits):\n",
    "    kfold=StratifiedKFold(n_splits=splits, random_state=7, shuffle= True)\n",
    "    model=SVC(gamma=\"scale\")\n",
    "    results=cross_val_score(model, x, y, cv=kfold)\n",
    "    print(f'Support Vector Machines - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "    return model,results\n",
    "              \n",
    "### Regression ###\n",
    "def evaluate_regressors(X, y, n_splits=10, shuffle=True, random_state=0, scoring='neg_mean_squared_error'):\n",
    "    \n",
    "    models = [('LinearRegression',      LinearRegression()),\n",
    "              ('Ridge',                 Ridge()),\n",
    "              ('Lasso',                 Lasso()),\n",
    "              ('ElasticNet',            ElasticNet()),\n",
    "              ('KNeighborsRegressor',   KNeighborsRegressor()),\n",
    "              ('DecisionTreeRegressor', DecisionTreeRegressor()),\n",
    "              ('SVR',                   SVR(gamma=\"auto\")),\n",
    "              ('XGBoostRegressor',      XGBRegressor(objective='reg:squarederror'))]\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    \n",
    "    results = []\n",
    "    for model in models:\n",
    "        res = cross_val_score(model[1], X, y, cv=kfold, scoring=scoring)\n",
    "        [results.append((model[0], r)) for r in res]\n",
    "    \n",
    "    results = pd.DataFrame(results, columns=['Model', 'Result'])\n",
    "    return results\n",
    "              \n",
    "def plot_regressors(results):         \n",
    "    plt.figure(figsize=(15,9))\n",
    "    chart = sns.boxplot(data=results, x='Model', y='Result')\n",
    "    chart = sns.swarmplot(data=results, x='Model', y='Result', color=\"royalblue\")\n",
    "    chart.set_xticklabels(labels=results['Model'].unique(), rotation=45, horizontalalignment='right')\n",
    "    print('Regression Results:')\n",
    "              \n",
    "### Ensembles ###\n",
    "def evaluate_ensembles(X, y, max_features=10, n_estimators=50, n_splits=10, shuffle=True, random_state=0):\n",
    "    models = [\n",
    "        ('BaggingClassifier',          BaggingClassifier(n_estimators=n_estimators,\n",
    "                                                         max_features=max_features, \n",
    "                                                         random_state=random_state)),\n",
    "        \n",
    "        ('RandomForestClassifier',     RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                                              max_features=max_features, \n",
    "                                                              random_state=random_state)),\n",
    "        \n",
    "        ('ExtraTreesClassifier',       ExtraTreesClassifier(n_estimators=n_estimators, \n",
    "                                                            max_features=max_features, \n",
    "                                                            random_state=random_state)),\n",
    "        \n",
    "        ('AdaBoostClassifier',         AdaBoostClassifier(n_estimators=n_estimators, \n",
    "                                                          random_state=random_state)),\n",
    "        \n",
    "        ('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                                                  max_features=max_features, \n",
    "                                                                  random_state=random_state))]\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    \n",
    "    results = []\n",
    "    for model in models:\n",
    "        res = cross_val_score(model[1], X, y, cv=kfold)\n",
    "        [results.append((model[0], r)) for r in res]\n",
    "    \n",
    "    results = pd.DataFrame(results, columns=['Model', 'Result'])\n",
    "    return results\n",
    "              \n",
    "### Hyperparameter Tuning ###\n",
    "              \n",
    "def grid_search(model, x,y,kfold):         \n",
    "    # GridSearch\n",
    "    parameters = {\n",
    "        'max_depth': [6, 7, 8, 9, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 150, 200]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, param_grid=parameters, cv=kfold)\n",
    "    grid.fit(X,y)\n",
    "\n",
    "    print(grid.best_score_)\n",
    "\n",
    "    # randomized search\n",
    "    parameters = {\n",
    "        'max_depth': randint(1, 8),\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': randint(50, 300),\n",
    "    }\n",
    "\n",
    "\n",
    "    random_grid = RandomizedSearchCV(estimator=model, \n",
    "                                     param_distributions=parameters, \n",
    "                                     n_iter=100, \n",
    "                                     cv=kfold, \n",
    "                                     random_state=0)\n",
    "    random_grid.fit(X,y)\n",
    "\n",
    "    print(random_grid.best_score_)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Import ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculations ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
