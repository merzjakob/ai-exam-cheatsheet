{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries ####\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import ipywidgets as widgets\n",
    "import qgrid\n",
    "import pandas_profiling\n",
    "import time\n",
    "import sklearn\n",
    "import math\n",
    "import chart_studio.plotly as py\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#Model Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "#Performance Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#clustering\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from yellowbrick.cluster import InterclusterDistance\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn import metrics\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Settings ####\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 400) \n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function Library ####\n",
    "\n",
    "def print_bold(string):\n",
    "    print('\\033[1m' + string )\n",
    "    print('\\033[0m')\n",
    "\n",
    "### Inital steps ###\n",
    "def examine_df(df):\n",
    "    print(df.info())\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    print(df.describe())\n",
    "    \n",
    "    return pandas_profiling.ProfileReport(df)\n",
    "\n",
    "def write_result_to_csv(df):\n",
    "    df.to_csv('result.csv', sep=',')\n",
    "    \n",
    "def distribution_plot(df, column_name):\n",
    "    sns.distplot(df[column_name])\n",
    "    return plt.figure()\n",
    "    \n",
    "def joint_plot(df,x,y):\n",
    "    return sns.jointplot(x = x, y=y, data = df, kind = 'reg')\n",
    "\n",
    "def pair_plot(df):\n",
    "    return sns.pairplot(df, kind = 'reg')\n",
    "\n",
    "### Pre-Processing ###\n",
    "\n",
    "def seperate_components(df, column_of_y):\n",
    "    print('Note: let x,y = function to define globally')\n",
    "    time.sleep(2)\n",
    "    array = df.values\n",
    "    if ((int(len(array[0]))) - 1) != column_of_y:\n",
    "        print('Warning: adapt function if y not last column!')\n",
    "    X = array[:,0:column_of_y]\n",
    "    Y = array[:,column_of_y]\n",
    "    return X,Y\n",
    "\n",
    "def rescale(x):\n",
    "    print('Note: let rescaledX,x_scaled_fit = rescale(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    x_scaled_fit = scaler.fit(x)\n",
    "    rescaledX=scaler.fit_transform(x)\n",
    "    return rescaledX, x_scaled_fit\n",
    "\n",
    "def standardize(x):\n",
    "    print('Note: let standardizedX = standardize(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    scaler= StandardScaler().fit(x)\n",
    "    rescaledX = scaler.transform(x)\n",
    "    return standardizedX\n",
    "\n",
    "def normalize(x):\n",
    "    print('Note: let normalizedX = normalize(X) to define globally')\n",
    "    time.sleep(2)\n",
    "    normalizedX = Normalizer().fit_transform(x)\n",
    "    return normalizedX\n",
    "\n",
    "def binarize(x,threshold):\n",
    "    print('Note: let binarizedX = binarize(X) to define globally and set threshold to value required')\n",
    "    time.sleep(2)\n",
    "    binaryX = Binarizer(threshold=threshold).fit_transform(x)\n",
    "    return binaryX\n",
    "\n",
    "def encode(df,name_of_column,new_name):\n",
    "    print('Note: let df = encode(X,name_of_column,new_name) to define globally')\n",
    "    time.sleep(2)\n",
    "    df[name_of_column]=LabelEncoder().fit_transform(df[new_name])\n",
    "    return df\n",
    "    \n",
    "def get_dummies(df, column_name):\n",
    "    print('Note: let df = get_dummies(df,name_of_column) to define globally')\n",
    "    time.sleep(2)\n",
    "    print(df[column_name].unique())\n",
    "    gen_features = pd.get_dummies(df[column_name],prefix = column_name, prefix_sep= '_',drop_first = True)\n",
    "    df = pd.concat([df,gen_features], axis=1)\n",
    "    df = df.drop([column_name], axis=1)\n",
    "    return df\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "def univariate_chi(x,y,df,target_var, k=4):\n",
    "    test = SelectKBest(score_func=chi2,k=k)\n",
    "    fit = test.fit(x,y)\n",
    "    print_bold('Univariate Scores')\n",
    "    score = list(fit.scores_)\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    results.loc[''] = score\n",
    "    print(f'The {k} attributes with highest scores are: ')\n",
    "    count = 1\n",
    "    while count <= k:\n",
    "        max_value = results.idxmax(axis=1)\n",
    "        print(f'{count}: ' + max_value.values)\n",
    "        results = results.drop(columns = max_value.values)\n",
    "        count += 1\n",
    "    print('------------')\n",
    "\n",
    "def recursive_elimination(x,y,df,target_var, k=3):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    \n",
    "    rfe = RFE(model,k)\n",
    "    fit = rfe.fit(x,y)\n",
    "    print_bold(f'Recursive Scores')\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    score = list(fit.ranking_)\n",
    "    score = list(map(int, score))\n",
    "    results = pd.DataFrame(columns = columns)\n",
    "    results.loc[''] = score\n",
    "    print(f'The {k} attributes with highest scores are: ')\n",
    "    count = 1\n",
    "    while count <= k:\n",
    "        min_value = results.astype('float64').idxmin(axis=1)\n",
    "        print(f'{count}: ' + min_value.values)\n",
    "        results = results.drop(columns = min_value.values)\n",
    "        count += 1\n",
    "    print('------------')\n",
    "    \n",
    "def pca(x,k=3):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca_fit = pca.fit(x)\n",
    "    print(f\"Explained variance: {pca_fit.explained_variance_ratio_}\")\n",
    "    print()\n",
    "    print(\"Principal Components have little resemblance to the source data attributes\")\n",
    "    print()\n",
    "    print(pca_fit.components_)\n",
    "\n",
    "def extra_trees(x,y,df,target_var,estimators=100):\n",
    "    model = ExtraTreesClassifier(n_estimators=estimators)\n",
    "    model.fit(x,y)\n",
    "    print_bold('Feature Importance Scores')\n",
    "    score = list(model.feature_importances_)\n",
    "    columns = (list(df.columns.values))\n",
    "    columns.remove(target_var)\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    results.loc[''] = score\n",
    "    np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "    print(f'The importance of attributes in descending order: ')\n",
    "    print()\n",
    "    print(round((results.max().sort_values(ascending=False)),3))\n",
    "    print('------------')\n",
    "    \n",
    "### Model Evaluation ###\n",
    "\n",
    "def similarity_of_split(train,test,target_var):\n",
    "    print('TBD')\n",
    "    \n",
    "def test_split(x,y,test_size, seed=7):\n",
    "    #print('Note: let X_train, X_test, Y_train, Y_test = test_split(X,Y, 0.3) to define globally')\n",
    "    time.sleep(2)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size = test_size,random_state = 7)\n",
    "    \n",
    "    # Let's do the log regresssion\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_train,Y_train)\n",
    "\n",
    "    # Now let's find the accurary with the test split\n",
    "    result = model.score(X_test, Y_test)\n",
    "    print(f'Test: {test_size} Train: {1-test_size}')\n",
    "    print(f'Accuracy {result*100:5.3f}')\n",
    "    print('-------------')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def k_fold_cross_val(x,y,splits=10,scoring = 'accuracy',add_info= False):\n",
    "    kfold= KFold(n_splits=splits, random_state=7)\n",
    "    model= LogisticRegression(solver=\"liblinear\")\n",
    "    if add_info:\n",
    "        scoring = {'accuracy': 'accuracy',\n",
    "           'recall': 'recall',\n",
    "           'precision': 'precision',\n",
    "           'f1': 'f1'}\n",
    "        results = cross_validate(model, x, y, scoring=scoring, cv=kfold)\n",
    "        print(f'Logistic regression, k-fold {splits:d}')\n",
    "        print(f'Accuracy {results[\"test_accuracy\"].mean()*100:.3f}%')\n",
    "        print(f'Precision {results[\"test_precision\"].mean()*100:.3f}%')\n",
    "        print(f'Recall {results[\"test_recall\"].mean()*100:.3f}%')\n",
    "        print(f'F1 {results[\"test_f1\"].mean()*100:.3f}%')\n",
    "    else:\n",
    "        scoring=scoring\n",
    "        results = cross_val_score(model, x,y,cv=kfold,scoring=scoring)\n",
    "        print(f'Logistic regression, k-fold {splits:d} - {scoring}')\n",
    "        if scoring == 'accuracy':\n",
    "            print(f'{results.mean()*100:5.3f}% ({results.std()*100:5.3f}%)')\n",
    "        else:\n",
    "            print(f'{results.mean():5.3f}')\n",
    "    return model\n",
    "\n",
    "              \n",
    "def leave_one_out(x,y):\n",
    "    loo= LeaveOneOut()\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    \n",
    "    results = cross_val_score(model, x,y,cv=loo)\n",
    "    print(f'Logistic regression, Leave one out - Accuracy {results.mean()*100:5.3f}% ({results.std()*100:5.3f}%)')\n",
    "\n",
    "def repeated_test_train(x,y, test_size,repetitions=10):\n",
    "    shuffle=ShuffleSplit(n_splits=repetitions,test_size=test_size, random_state=7)\n",
    "    model= LogisticRegression(solver=\"liblinear\")\n",
    "    res = cross_val_score(model,x,y,cv=shuffle)\n",
    "    \n",
    "    print(f'Log Regression - Repeated Test-Train {nrepeat:d} - Accuracy {res.mean()*100:5.3f}% {res.std()*100:5.3f}%')\n",
    "    \n",
    "\n",
    "### Performance Metrics ###\n",
    "\n",
    "def precision_recall_curve(x,y,test_size):\n",
    "    model = k_fold_cross_val(x,y,add_info=True)\n",
    "              \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "    \n",
    "    viz=PrecisionRecallCurve(model)\n",
    "    viz.fit(X_train, Y_train)\n",
    "    viz.score(X_test,Y_test)\n",
    "    viz.show()\n",
    "\n",
    "def area_under_roc(x,y,test_size):\n",
    "    model = k_fold_cross_val(x,y, scoring='roc_auc')\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "\n",
    "    viz=ROCAUC(model, classes=[0,1])\n",
    "    viz.fit(X_train, Y_train)\n",
    "    viz.score(X_test,Y_test)\n",
    "    viz.show()\n",
    "\n",
    "def con_matrix(x,y,test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    c_matrix=confusion_matrix(y_test, y_predicted)\n",
    "    print_bold(\"Confusion Matrix\")\n",
    "\n",
    "    print()\n",
    "    print(f'Accuracy {model.score(X_test, y_test)*100:.5f}')\n",
    "    print(f'Accuracy check with conf. matrix {(c_matrix[0,0]+c_matrix[1,1])/c_matrix.sum()*100:.5f}')\n",
    "\n",
    "    cm = ConfusionMatrix(model, classes=[\"Not present\",\"Present\"])\n",
    "    # cm.fit(X_train, y_train)  #only if the model is not fitted\n",
    "\n",
    "    cm.score(X_test, y_test)\n",
    "    cm.show()\n",
    "\n",
    "def class_report(x,y, test_size=0.3):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=7)\n",
    "    \n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_predicted = model.predict(X_test)\n",
    "\n",
    "    report = classification_report(Y_test, Y_predicted, digits=5)\n",
    "\n",
    "    print(f'Accuracy {model.score(X_test, Y_test)*100:.5f}')\n",
    "    print()\n",
    "    print(report)\n",
    "              \n",
    "def mean_abs_error(X,Y):\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    model = LinearRegression()\n",
    "    scoring = \"neg_mean_absolute_error\"\n",
    "    res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    print(f'Linear Regression, MAE: {res.mean():.3f} ({res.std():.3f})')\n",
    "\n",
    "def mean_squared_error(X,Y):\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    model = LinearRegression()\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    print(f'Linear Regression, MSE: {res.mean():.3f} ({res.std():.3f})')\n",
    "    print(f'Linear Regression, MSE: {math.sqrt(abs(res.mean())):.3f} ({math.sqrt(res.std()):.3f})')\n",
    "\n",
    "def r_2(X,Y):\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    model = LinearRegression()\n",
    "              \n",
    "    scoring = \"r2\"\n",
    "    res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    print(f'Linear Regression, R2: {res.mean():.3f} ({res.std():.3f})')\n",
    "\n",
    "### Clustering ###\n",
    "\n",
    "def scale_cluster_df(x):\n",
    "    x_scaled,x_scaled_fit = rescale(x)\n",
    "    X_scaled = pd.DataFrame(x_scaled,columns=x.columns)\n",
    "    return X_scaled,x_scaled_fit        \n",
    "              \n",
    "def elbow(x_scaled):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    model= KMeans()\n",
    "    \n",
    "    visualizer = KElbowVisualizer(model, k=(1,8))\n",
    "    visualizer.fit(x_scaled)       \n",
    "    visualizer.show() \n",
    "     \n",
    "def cluster_model(x_scaled, clusters):\n",
    "    model=MiniBatchKMeans(n_clusters=clusters).fit(x_scaled)\n",
    "    return model\n",
    "              \n",
    "def silhoutte_visual(x_scaled, model):\n",
    "    plt.figure(figsize=(12,9))\n",
    "\n",
    "    visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n",
    "    visualizer.fit(x_scaled)      \n",
    "    visualizer.show()\n",
    "\n",
    "def inter_cluster_dist(x_scaled, model):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    visualizer = InterclusterDistance(model, min_size=10000)\n",
    "    visualizer.fit(x_scaled)\n",
    "    visualizer.show()     \n",
    "\n",
    "def accuracy_clustering(x_scaled, y, model):\n",
    "    model=MiniBatchKMeans(n_clusters=3)\n",
    "\n",
    "    model.fit(X_scaled)\n",
    "    \n",
    "    print(\" ---- Accuracy Scores ----\")\n",
    "\n",
    "    acc_score=accuracy_score(y.values,model.predict(x_scaled))\n",
    "    print(f'Accuracy {acc_score*100:.3f}')\n",
    "\n",
    "def silh_sco(x_scaled, model):\n",
    "    she=silhouette_score(x_scaled, model.labels_, metric='euclidean')\n",
    "    print(f'Silhouette score {she:5f}')\n",
    "\n",
    "def centroid_meaning(model, x, x_scaled):\n",
    "    model.labels_\n",
    "    model.cluster_centers_\n",
    "\n",
    "    centroids_rescaled = pd.DataFrame(model.cluster_centers_, columns=x.columns)\n",
    "    # rescale to original\n",
    "    centroids_original = pd.DataFrame(x_scaled_fit.inverse_transform(model.cluster_centers_),columns=x.columns)\n",
    "    print_bold('Originally scaled centroids')\n",
    "    return centroids_original\n",
    "              \n",
    "def aggl_cluster(x_scaled,clusters,y):\n",
    "    plt.figure(figsize=(17,9))\n",
    "\n",
    "    # create dendrogram\n",
    "    dn = sch.dendrogram(sch.linkage(x_scaled, method='ward'), no_labels=True)\n",
    "    plt.show()\n",
    "    # create clusters\n",
    "    hc = AgglomerativeClustering(n_clusters=clusters, affinity = 'euclidean', linkage = 'ward')\n",
    "    y_hc = hc.fit_predict(X_scaled)\n",
    "    hc.labels_\n",
    "    y.values\n",
    "    dk={0:2,1:0,2:1}\n",
    "    acc_score=accuracy_score(list(map(lambda x:dk[x],y.values)),hc.labels_)\n",
    "    print(f'Accuracy {acc_score*100:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Import ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculations ### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
